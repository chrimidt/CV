\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\usepackage{booktabs}



\begin{document}
\title{Computer Vision\\ \vspace{.5em} 
\Large Multi-label Image Classification}
\author{Group 25 \\ \vspace{.2em} Riajul Islam, Andreas Calonius Kreth, Christine Midtgaard}



\maketitle

% Todo:
% [] Describe the multi-label classification problem and its challenges.

% [] Explain each method's theoretical background and mechanisms.

% [] Compare their approaches to the problem.

% [] Reproduce their models, train and evaluate them.

% [] Compare your results with those reported in the papers.

% [] Evaluate the feasibility and performance of each method.

\begin{abstract}
Here is a summary of the project and the conclusions.
\end{abstract}

\begin{IEEEkeywords}
Multi-label learning, deep learning, computer vision, multi-label
classification, deep learning for MLC. 

\end{IEEEkeywords}

% ======================== Introduction ========================

\section{Introduction}

% [] Introduce the multi-label image classification problem and its challenges.

% [] Mention the two methods you investigate.

% [] Explain that your aim is to reproduce the results and evaluate their performance on MS-COCO.

% [] Clarify that the methods are not compared directly, but evaluated independently against the reported baselines.


Multi-label classification is the supervised learning problem where an instance may be associated with multiple labels. Image classification is a computer vision task that requires assigning a label or multiple labels to an image. Single-label classification, or multi-class classification, refers to the problem where an image contains only one object to be identified. However, natural images usually contain multiple objects or concepts, highlighting the importance of multi-label classification \cite{ridnik2021mldecoderscalableversatileclassification}. In this project we investigate two methods aimed to solve two different problems within multi-label learning: Query2Label targets the challenges of imbalance and object localization, whereas MLSPL addresses the challenge of training an effective multi-label classifier from minimal supervision.

\subsection{Multi-Label Classification Problem}
% ======================== Related Work ========================

\section{Related Work}
Multi-label learning is a well studied problem within computer vision \cite{mlsp}. 

% [] Provide a concise review of existing methods in multi-label classification.

% [] Include references to transformer-based methods (like Q2L), and weakly supervised learning methods (like MLSPL).

\paragraph{Loss Functions}

\paragraph{Convolutional Neural Networks}

\paragraph{Vision Transformer}


\paragraph{PU learning} 
Learning from positive and unlabeled data: a survey by Bekker and Davis \cite{Bekker_2020}.

Learning to Classify Texts Using Positive and Unlabeled Data by Li and Liu \cite{Li_2003}.

% ======================== Theoretical Background ========================

\section{Theoretical Background}
The theory behind multi-label classification and the methods in the chosen papers.

\subsection{Multi-label Classification}
Write about the theory behind multi-label classification/learning.

\subsection{Query2Label: A Simple Transformer Way to Multi-Label Classification}
Query2Label: A Simple Transformer Way to Multi-Label Classification (Query2Label) by Liu et al. \cite{Query2Label} is a two-stage framework for multi-label classification. It uses transformer decoders to extract features with multi-head attentions focusing on different parts of an object category and learn label embeddings from data automatically.

\paragraph{Backbone architecture}
The CvT by Wu et al. \cite{CvT}.

\subsection{Multi-Label Learning from Single Positive Labels}
Multi-Label Learning from Single Positive Labels (MLSPL) by Cole et al. \cite{mlsp}.
How it uses weak supervision and contrastive learning.

% ======================== Methodology ========================

\section{Methodology}
Focus Methodology on the rationale for choosing these methods, how the operationalized their training, and any adaptations made for reproduction.

\subsection{Overview of Approach}
A brief description of what we did to solve the multi-label classification problem. 

\subsection{Dataset}
The MS-COCO 2014 \cite{coco14} dataset is used as a benchmark for evaluation both Query2Label and MLSPL. MS-COCO (Microsoft Common Objects in Context) is a large-scale dataset commonly used for object detection, segmentation, and multi-label image classification. COCO consists of 82,081 training images and 80 classes, and a validation set of 40,137 images.

\subsection{Query2Label}
Write about why we chose Query2Label for solving the multi-label classification problems.

\subsection{MLSPL}
Write about why we chose MLSPL for solving the multi-label classification problem.


% ======================== Experimental Setup  ========================

\subsection{Experiments}
A description of our setup versus the author's setups.

\vspace{1em}

All experiments were run on a single NVIDIA GeForce RTX 3060 GPU (12 GB VRAM), with Python 3.11.8, NVIDIA driver 550.90.07 and CUDA 12.4. 


\subsubsection{Query2Label}
All Query2Label experiments were conducted on environment versions cuda==12.4, torch==2.1.0+cu121, torchvision==0.16.0+cu121, python==3.11.8.
We evaluated the Query2Label model using the best performing backbone model, the CvT-w24 backbone, with a $384\time 384$ input resolution, pretrained on the ImageNet-22k dataset, as described by Liu et al. \cite{Query2Label}. For our experiments, we used a pretrained model checkpoint released by the authors, which was trained on the MS-COCO 2024 dataset. The model was tested on the MS-COCO 2024 validation set with a batchsize of 16 and otherwise no fine-tuning or modification.

% \paragraph{Data preparation}
% \paragraph{Hyperparameters}

\subsubsection{Multi-Label Learning from Single Positive Labels}
All MLSPL experiments were conducted on environment versions cuda==12.4, torch==2.2.1+cu121, torchvision==0.17.1+cu121, python==3.11.8.
\paragraph{Data preparation}
The dataset preparation for the MS-COCO dataset in MLSPL relies on converting the standard multi-label MS-COCO dataset to a single positive label format, simulating a weakly supervised setting. Cole et al. does this beginning with the fully annotated multi-label image dataset and corrupt it by discarding annotations, simulating single positive training data by randomly selecting one positive label for each training example \cite{mlsp}.

\vspace{1em}

To convert into a single positive label format, the instructions provided by Cole et al. are followed: firsly, both images and annotations for training and validation are downloaded. Secondly, pre-extracted features for COCO, provided by the authors are downloaded. Lastly, the script \texttt{format\_coco.py} is used to produce uniformly formatted image lists and labels.

\vspace{1em}

\paragraph{Hyperparameters}

\subsection{Evaluation Metrics}
To asses model performance, we adopt mean Average Precision (mAP), a standard metric widely reported in multi-label classification tasks as it is used to analyze the performance object detection and segmentation. Both Query2Label and MLSPL report results in terms of mAP. 

% ======================== Results and Discussion ========================

\section{Results and Discussion}
This section compares results from the experiments to the those of the papers. Discuss why they might not be the same, or describe the similarities. Discuss whether the methods are able to solve the problems.


\begin{table*}[!t]
    \small
    \caption{Comparison of mAP results retween our experiments and reported mAP results on the MS-COCO 2014 Dataset.}
    \label{tab:map_comparison}
    \centering
    \begin{tabular}{l l l c c}
    \toprule
    \textbf{Method} & \textbf{Backbone} & \textbf{Resolution} & \textbf{mAP(Ours)} & \textbf{mAP (Paper)} \\
    \midrule
    Q2L-R101-448 & ResNet-101     & $448\times448$ & 84.9 & 84.9 \\
    Q2L-R101-576 & ResNet-101     & $576\times576$ & 86.5 & 86.5 \\
    Q2L-SwinL    & Swin-L(22k)    & $384\times384$ & 90.5 & 90.5 \\
    Q2L-CvT      & CvT-w24(22k)   & $384\times384$ & 91.3 & 91.3 \\
    \bottomrule
    \end{tabular}
\end{table*}


\subsection{Query2Label}

\subsection{Multi-Label Learning from Single Positive Labels}

% ======================== Conclusion ========================

\section{Conclusion}
% [] Summarize findings for each method.

% [] Mention if the results reproduced align with the papers.

% [] Suggest future directions or improvements.


\bibliographystyle{IEEEtran}
\bibliography{references}




\end{document}


