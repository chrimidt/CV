\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{bbm}




\begin{document}
\title{Computer Vision\\ \vspace{.5em} 
\Large Multi-label Image Classification}
\author{Group 25 \\ \vspace{.2em} Riajul Islam, Andreas Calonius Kreth, Christine Midtgaard}



\maketitle

% Todo:
% [] Describe the multi-label classification problem and its challenges.

% [] Explain each method's theoretical background and mechanisms.

% [] Compare their approaches to the problem.

% [] Reproduce their models, train and evaluate them.

% [] Compare your results with those reported in the papers.

% [] Evaluate the feasibility and performance of each method.

\begin{abstract}
Abstract goes here.
\end{abstract}

\begin{IEEEkeywords}
Multi-label learning, deep learning, computer vision, multi-label
classification, deep learning for MLC. 

\end{IEEEkeywords}

% ======================== Introduction ========================

\section{Introduction}

% [] Introduce the multi-label image classification problem and its challenges.

% [] Mention the two methods you investigate.

% [] Explain that your aim is to reproduce the results and evaluate their performance on MS-COCO.

% [] Clarify that the methods are not compared directly, but evaluated independently against the reported baselines.


Multi-label classification is the supervised learning problem where an instance may be associated with multiple labels. Image classification is a computer vision task that requires assigning a label or multiple labels to an image. Single-label classification, or multi-class classification, refers to the problem where an image contains only one object to be identified. However, natural images usually contain multiple objects or concepts, highlighting the importance of multi-label classification \cite{ridnik2021mldecoderscalableversatileclassification}. 

In this project we investigate methods aimed to solve two common issues that arise when training a network that assigns multiple labels to an input image: (i) the general multi-label learning issue accurately identifying multiple objects in an image under class imbalance, representing the complexity of real-world images, and (ii) the scenario where the training data underwent sparse supervision.


\subsection{Problem Overview}

\paragraph{Label Correlation}
\paragraph{Class Imbalance}
\paragraph{Sparse Supervision}


\section{Related Work}
Multi-label learning is a well studied problem within computer vision \cite{mlsp}. 

% [] Provide a concise review of existing methods in multi-label classification.

% [] Include references to transformer-based methods (like Q2L), and weakly supervised learning methods (like MLSPL).

\paragraph{Loss Functions}

\paragraph{PU learning} 
Learning from positive and unlabeled data: a survey by Bekker and Davis \cite{Bekker_2020}.

Learning to Classify Texts Using Positive and Unlabeled Data by Li and Liu \cite{Li_2003}.

\paragraph{Partially Observed Labels}
% Learning a Deep ConvNet for Multi-label Classification with Partial Labels https://arxiv.org/pdf/1902.09720

\paragraph{Heuristics}
Heuristics can be used to reduce the required
annotation effort [34, 18], but this runs the risk of increasing
error in the labels \cite{mlsp}.

% ======================== Theoretical Background ========================
\section{Background}
This section describes relevant background theory related to the project and methods from MLSPL and Query2Label. Beginning with the definition of multi-label learning, followed by deep neural network architectures used to solve multi-label slassification tasks, and, finally, loss functions as they can handle challenges of label imbalance tasks.

\subsection{Multi-Label Learning}
Contrary to binary or multi-class classification where each instance is associated with only one label, multi-label classification allows assigning multiple labels to a single input \cite{mlsp}. Given $K$ categories, an input image $x\in \mathcal{X}$ is associated with a binary vector of labels $y=[y_1,...,y_K]$ from the label space $\mathcal{Y}=\{0,1\}^K$, where $y_k=1$ represents that $k$ is present in $x$ and $y_k=0$ otherwise. The goal is to create a model that outputs the probability of the presence of a category $p=[p_1,...,p_K]$ \cite{mlsp,Query2Label}.

\subsection{Convolutional Neural Networks (CNNs)}
In the field of computer vision, Convolutional Neural Networks (CNNs) have been a dominant approach for image analysis tasks since their introduction \cite{lecun95}. CNNs are designed to recognize patterns in images by applying local filters through convolutional layers, preserving the two dimensional input of an image \cite{zhang2023dive}. CNNs consist of three main types of layers: convolutional layers, pooling layers, and a Fully Connected (FC) layer \cite{asawaCS231n}. Convolutional layers extract spatial features through learnable filters, pooling layers reduce dimensionality and summarise information, while the fully connected layer at the end interpret the features to perform the classification.

\subsection{Transformer Architectures}
Transformers, introduced by Vaswani et al. \cite{vaswani2023attentionneed}, are a type of neural network architecture initially developed to model long-range dependencies in sequence data. They achieve this using attention and self-attention mechanisms, which enables the model to have a long-term memory of inputs, and dynamically weigh the importance of each element in the input sequence. Originally, transformer-based models was mainly delevoped for natural language processing tasks, but the introduction of the Vision Transformer (ViT) by Dosovitskiy et al. \cite{dosovitskiy2021imageworth16x16words}, transformers have been widely used for computer vision tasks. 

\paragraph{Architecture}
The original transformer consists of an encoder-decoder structure. Both encoder and decoder are made of a stack of identical layers, where encoder layers contain a multi-head self-attention mechanism followed by a position-wise feed-forward network. In addition, decoder layers consists of an encoder-decoder attention layer. 

The transfomer encoder is of interest, as the authors of \cite{Query2Label} use them to extract features and automatically learn label embedding, as described in section \ref{sec:q2l_method}.

\paragraph{Attention}
The core principle of the transformer architecture is the attention mechanism, which allows the model to attend to all posistions in an input sequence when processing each element. Thus alloowing the model to weigh the relevance of different posistions in a sequence. Given a query and a set of key-value pairs, the attention function computes a weighted sum of the values, where the weights are determined by similarities between the query and the keys.

\paragraph{Self-Attention}
The transformer model uses self-attention by relating every element in the input sequence to every other element. The attention function is a function that maps a query and a set of key-value pairs to an output. The scaled dot-product self-attention is defined as:

\begin{equation}
    \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

\noindent where $Q$, $K$, $V$, are the query, key, and value vectors, and $d_k$ is the dimension of the key vectors and serves as a scaling factor \cite{vaswani2023attentionneed}. 

\paragraph{Multi-Head Attention}
The transformer applies multiple attention functions in parallel, allowing the model to attend to information from different parts of the sequence. The embedding is split into multiple heads, perform attention for each, and then concatenate them back together, defined as:

\begin{equation}
    \text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\end{equation}
    
\noindent where each head is computed as:

\begin{equation}
    \text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)
\end{equation}

\noindent with projection matrices $W_i^Q$, $W_i^K$, $W_i^V$, and $W_i^O$.

\paragraph{Cross-Attention}
Contrary to self-attention, where queries, keys, and values are generated from the same input, cross-attention is a mechanisn where queries come from one source, and the keys and values come fron another. This mechanism allows the model to relate elements from one input to relevant parts of another input. The authours of Query2Label \cite{Query2Label} make use of this mechanism, where label embeddings (queries) attend to spatial image features (key-value pairs).

\paragraph{Feed-Forward Networks and Positional Encoding}
Each layer in the Transformer also includes a fully connected feed-forward network applied to each position. To compensate for the lack of order in the input sequence, sinusoidal positional encodings are added to the input embeddings. These encodings allow the model to distinguish the order of elements in the sequence using functions of varying frequencies.

\paragraph{Vision Transformers (ViTs)}
The Vision Transformer (ViT), introduced by Dosovitskiy et al. \cite{dosovitskiy2021imageworth16x16words}, is a model for image classification that leverages the transformer architecture: the self-attention mechanism of transformers that allows the model to selectively weigh the significance of each part of the input, and positional embeddings that represent the order of tokens in a sequence. In ViTs, images are represented as sequences, where the label function as a learnable token for classification. The input image is divided into a sequence of patches that are flattened and linearly embedded into a vector. The spatial information is preserved by adding positional encodings to the embeddings. The resulting sequence is fed into a transformer decoder identical to that introduced in \cite{vaswani2023attentionneed} to model global relations for classification.

\subsection{Loss Functions}
Loss functions are a fundamental component in deep learning as they serve as a measure of how much the model predictions deviate from the ground truth, guiding optimization of the network \cite{zhang2023dive}. The goal of optimization is to minimize the loss. In the context of multi-label learning, each instance can belong to multiple classes simumtaneously, and selecting an appropriate loss function is crucial. This section aims to decribe different loss functions employed in multi-label classification, and how they can be used to solve common issues in multi-label learning.

\paragraph{Binary Cross-Entropy (BCE)}
Due to its effectiveness in handling binary decisions for each class, a common choice of loss function for multi-label classification is the The Binary Cross-Entropy (BCE) loss \cite{mlsp,durand2019learningdeepconvnetmultilabel,nayan2024binary}. In binary classification, the goal is to classify data as either positive or negative, represented as 0 or 1. The output of the model is a probability score between 0 and 1, indicating the probability of the input belonging to the positive class \cite{nayan2024binary}.

The BCE loss for multi-label classification is given by:

\begin{equation}
\begin{aligned}
\mathcal{L}_{\text{BCE}} = -\frac{1}{K} \sum_{i=1}^{K} \bigl[ &y_i\log(p_i) \\
+ &(1-y_i)\log(1 - p_i) \bigr]
\end{aligned}
\end{equation}

\noindent where $K$ is the number of categories, $y_i\in[0,1]$ is the binary label, and $p_i$ is the predicted probability.

However, BCE loss can be sensitive to class imbalance, where some classes appear more often than others. In this case, the model tend to focus on the majority class and perform poorly on the minority class. 

\paragraph{Loss Functions for the Partially Observed Labels Setting}
BCE assumes that each training example indicates the presence or absence of each class, e.i. the example is fully observable. In real-world multi-label datasets, it is often infeasible to annotate all relevant labels for each image, resulting in partially observed labels. In the case where labels are only partially observed, the assumption of full negative labeling in BCE can lead to false negatives during training, degrading model performance \cite{mlsp}. 

To address this issue, several loss functions have been proposed to handle partial supervision. In this project, we explore the case of minimal supervision: the single positive label setting, where only one relevant class label is observed per image and the rest are unannotated. 

\paragraph{Loss Functions for the Single Positive Label Setting}
In the single positive label setting, traditional loss functions such as BCE and its naive adaptations are insufficient, as they rely on the assumption that unobserved labels are negatives. This leads to a high rate of false negatives, especially when only one positive label per image is observed. Several variants have been proposed to mitigate this, including the Assume Negative loss, Weak Assume Negative loss, and Expected Positive Regularization. While these approaches relax the assumptions on the unobserved labels, they still depend on fixed heuristics.

To address these limitations, the authors of \cite{mlsp} propose a more flexible and data-driven approach: Regularized Online Label Estimation (ROLE). This method jointly learns the classifier and a dynamic label estimator, which maintains soft estimates of the full label vector during training. The key insight is that by encouraging consistency between predictions, known labels, and an expected number of positives per image, the model can recover many of the unobserved true labels.

\paragraph{Regularized Online Label Estimation (ROLE)}
ROLE introduces a second network (or parameter matrix) to estimate the unobserved labels online during training. These soft label estimates are updated alongside the main classifier using a bi-level optimization procedure, where each component reinforces the other. The training objective is composed of three terms:
\begin{itemize}
\item A binary cross-entropy loss between model predictions and estimated labels.
\item A BCE loss over observed positive labels (like in AN).
\item A regularization term that constrains the number of predicted positives per image to match a predefined expected count $k$.
\end{itemize}

The ROLE loss is given by:

\begin{equation}
    \mathcal{L}_{ROLE}(\mathbf{F}_B, \mathbf{\tilde{Y}}_B) = \frac{\mathcal{L}'(\mathbf{F}_B)|\mathbf{\tilde{Y}}+\mathcal{L}'(\mathbf{\tilde{Y}}|\mathbf{F}_B)}{2}
\end{equation}

\noindent for a batch $B$, where $\mathbf{\tilde{Y}}\in[0,1]^{N\times L}$ represent the estimated labels, and $\mathbf{F}\in[0,1]^{N\times L}$ is the matrix of classifier predictions.

% The goal is to train the label estimator $g(\cdot;\phi)$ and the image classifier $f(\cdot;\theta)$

By refining the label estimates online and regularizing their distribution, ROLE significantly improves robustness to missing labels and has been shown to match or even outperform some fully supervised baselines under extreme label sparsity.

% \paragraph{Focal Loss}

% \paragraph{Assymetric Loss}


% ======================== Methodology ========================

\section{Method}
% Focus Methodology on the rationale for choosing these methods, how the operationalized their training, and any adaptations made for reproduction.

In real-world datasets, obtaining full label annotations is practiaclly impossible.
This project focuses on finding solutions to the multi-label learning problems:
\begin{itemize} 
    \item  The challenge of multi-label learning in scenarios where only a single positive label per image is available durin training (MLSPL).
    \item Label imbalance (Query2Label).
    \item Feature localization (Query2Label).
\end{itemize}

\subsection{Overview of Approach}
A brief description of what we did to solve the multi-label classification problem. 



\subsection{Query2Label: A Simple Transformer Way to Multi-Label Classification}
\label{sec:q2l_method}
Query2Label: A Simple Transformer Way to Multi-Label Classification (Query2Label) by Liu et al. \cite{Query2Label} is a two-stage framework for multi-label classification. It uses transformer decoders to extract features with multi-head attentions focusing on different parts of an object category and learn label embeddings from data automatically.

\subsection{Multi-Label Learning from Single Positive Labels}
Multi-Label Learning from Single Positive Labels (MLSPL) by Cole et al. \cite{mlsp}.

The authors' solution to a sparsely annotated training dataset is twofold: first, they develop a training methodology using datasets where annotators have only provided a single confirmed positive label per image, with no confirmed negatives. Second, they extend existing multi-label loss functions to handle this challenging scenario and introduce novel variants to constrain the expected number of positive labels.




% ======================== Experimental Setup  ========================

\section{Experiments}
All experiments were conducted on a cluster with Ubuntu 22.04.5 LTS, a single NVIDIA GeForce RTX 3060 GPU (12 GB VRAM), with Python 3.11.8, NVIDIA driver 550.90.07 and CUDA 12.4. 

\subsection{Dataset}
The MS-COCO 2014 \cite{coco14} dataset is used as a benchmark for evaluation both Query2Label and MLSPL. MS-COCO (Microsoft Common Objects in Context) is a large-scale dataset commonly used for object detection, segmentation, and multi-label image classification. COCO consists of 82,081 training images and 80 classes, and a validation set of 40,137 images.

\begin{figure}[t]
    \centering
    \includegraphics[width=.8\linewidth]{images/coco_grid.png}
    \caption{Examples from the MS-COCO 2014 training set, resized to $448 \times 448$ pixels.}
    \label{fig:coco-examples}
\end{figure}


\subsection{Query2Label}
All Query2Label experiments were conducted on environment versions cuda==12.4, torch==2.1.0+cu121, torchvision==0.16.0+cu121, python==3.11.8.
We evaluated the Query2Label model on the four backbone architectures: ResNet-101 with a resolution of $448\times 448$, ResNet-101 with a resolution of $576\times 576$, Swin-L(22k) and CvT-w24(22k), both with a resolution of $384\times 384$ and pre-trained on ImageNet-22k \cite{imagenet} dataset, as described by Liu et al. \cite{Query2Label}. For our experiments, we used pretrained model checkpoints released by the authors, trained on the MS-COCO 2014 dataset. The model was tested on the MS-COCO 2024 validation set with a batchsize of 16 and otherwise no fine-tuning or modification.

\paragraph{Limitations}
Due to limited time, it was not possible to train the model from scratch, hence why we used pretrained weights to conduct the experiments on the Query2Label method.

% \paragraph{Data preparation}
% \paragraph{Hyperparameters}

\subsection{Multi-Label Learning from Single Positive Labels}
All MLSPL experiments were conducted on environment versions cuda==12.4, torch==2.2.1+cu121, torchvision==0.17.1+cu121, python==3.11.8.
\paragraph{Data preparation}
The dataset preparation for the MS-COCO dataset in MLSPL relies on converting the standard multi-label MS-COCO dataset to a single positive label format, simulating a weakly supervised setting. Cole et al. does this beginning with the fully annotated multi-label image dataset and corrupt it by discarding annotations, simulating single positive training data by randomly selecting one positive label for each training example \cite{mlsp}.

To convert into a single positive label format, the instructions provided by Cole et al. are followed: firsly, both images and annotations for training and validation are downloaded. Secondly, pre-extracted features for COCO, provided by the authors are downloaded. Lastly, the script \texttt{format\_coco.py} is used to produce uniformly formatted image lists and labels.

\paragraph{Hyperparameters}

\subsection{Evaluation Metrics}
To asses model performance, we adopt mean Average Precision (mAP), a standard metric widely reported in multi-label classification tasks as it is used to analyze the performance object detection and segmentation. Both Query2Label and MLSPL report results in terms of mAP. 
\begin{table*}[t]
    \small
    \caption{Comparison of mAP results from our experiments and the reported results on Query2Label on the MS-COCO 2014 Dataset.}
    \label{tab:q2l_map_comparison}
    \centering
    \begin{tabular}{l l l c c}
    \toprule
    \textbf{Method} & \textbf{Backbone} & \textbf{Resolution} & \textbf{mAP(Ours)} & \textbf{mAP (Paper)} \\
    \midrule
    Q2L-R101-448 & ResNet-101     & $448\times448$ & 84.9 & 84.9 \\
    Q2L-R101-576 & ResNet-101     & $576\times576$ & 86.5 & 86.5 \\
    Q2L-SwinL    & Swin-L(22k)    & $384\times384$ & 90.5 & 90.5 \\
    Q2L-CvT      & CvT-w24(22k)   & $384\times384$ & 91.3 & 91.3 \\
    \bottomrule
    \end{tabular}
\end{table*}

% ======================== Results and Discussion ========================

\section{Results and Discussion}

This section compares results from the experiments to the those of the papers. Discuss why they might not be the same, or describe the similarities. Discuss whether the methods are able to solve the problems.


\subsection{Results from Query2Label}
The results from our experiments are compared to those of the paper in table \ref{tab:q2l_map_comparison}. 



\subsection{Results from Multi-Label Learning from Single Positive Labels}
The results from our experiments are compared to those of the paper in table \ref{tab:MLSPL_map_comparison}. 

\begin{table}[h]
    \small
    \caption{Comparison of mAP results from our experiments and the reported results on Multi-Label Learning from Single Positive Labels on the MS-COCO 2014 Dataset.}
    \label{tab:MLSPL_map_comparison}
    \centering
    \begin{tabular}{l l c c}
    \toprule
    \textbf{Loss} & \textbf{Method} & \textbf{mAP(Ours)} & \textbf{mAP (Paper)} \\
    \midrule
    $\mathcal{L}_{ROLE}$ & Linear & 66.3 & 66.3 \\
    $\mathcal{L}_{ROLE}$ & Fine-Tuned & 66.9 & 66.3 \\
    \bottomrule
    \end{tabular}
\end{table}

Linear: batchsize: 16, lr: 0.001. Validation set variant: clean. Seed 1200. Train set variant: observed. Arch: linear.
Finte-tuned: Batchsize: 16, lr: 0.00001 on the fine-tuned. Validation set variant: clean. Seed 1200. Train set variant: observed. Arch: resnet50.

% ======================== Conclusion ========================

\section{Conclusion}

Conclusion here.
% [] Summarize findings for each method.

% [] Mention if the results reproduced align with the papers.

% [] Suggest future directions or improvements.


\bibliographystyle{IEEEtran}
\bibliography{references}




\end{document}


