\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{bm}




\begin{document}
\title{Computer Vision\\ \vspace{.5em} 
\Large Multi-label Image Classification}
\author{Group 25 \\ \vspace{.2em} Riajul Islam, Andreas Calonius Kreth, Christine Midtgaard}



\maketitle

% Todo:
% [] Describe the multi-label classification problem and its challenges.

% [] Explain each method's theoretical background and mechanisms.

% [] Compare their approaches to the problem.

% [] Reproduce their models, train and evaluate them.

% [] Compare your results with those reported in the papers.

% [] Evaluate the feasibility and performance of each method.

\begin{abstract}
    We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision
    Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer
    block leveraging a convolutional projection. These changes
    introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale,
    and distortion invariance) while maintaining the merits of
    Transformers (i.e. dynamic attention, global context, and
    better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves
    state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains
    are maintained when pretrained on larger datasets (e.g.
    ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7\% on the ImageNet-1k val set. Finally, our
    results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely removed in our model, simplifying the design for higher resolution vision tasks.
\end{abstract}

\begin{IEEEkeywords}
Multi-label learning, deep learning, computer vision, multi-label
classification, deep learning for MLC. 

\end{IEEEkeywords}

% ======================== Introduction ========================

\section{Introduction}

% [] Introduce the multi-label image classification problem and its challenges.

% [] Mention the two methods you investigate.

% [] Explain that your aim is to reproduce the results and evaluate their performance on MS-COCO.

% [] Clarify that the methods are not compared directly, but evaluated independently against the reported baselines.


Multi-label classification is the supervised learning problem where an instance may be associated with multiple labels. Image classification is a computer vision task that requires assigning a label or multiple labels to an image. Single-label classification, or multi-class classification, refers to the problem where an image contains only one object to be identified. However, natural images usually contain multiple objects or concepts, highlighting the importance of multi-label classification \cite{ridnik2021mldecoderscalableversatileclassification}. In this project we investigate two methods aimed to solve two different problems within multi-label learning: Query2Label targets the challenges of imbalance and object localization, whereas MLSPL addresses the challenge of training an effective multi-label classifier from minimal supervision.

This project focuses on testing existing solutions to two common issues that arise when training a network that assigns multiple labels to an input image: (i) the general multi-label learning issue accurately identifying multiple objects in an image under class imbalance, representing the complexity of real-world images, and (ii) the scenario where the training data underwent sparse supervision, which is often the case for data.

% ======================== Problem Overview ========================
\section{Problem Overview}

\subsection{Related Work}
Multi-label learning is a well studied problem within computer vision \cite{mlsp}. 

% [] Provide a concise review of existing methods in multi-label classification.

% [] Include references to transformer-based methods (like Q2L), and weakly supervised learning methods (like MLSPL).

\paragraph{Loss Functions}

\paragraph{PU learning} 
Learning from positive and unlabeled data: a survey by Bekker and Davis \cite{Bekker_2020}.

Learning to Classify Texts Using Positive and Unlabeled Data by Li and Liu \cite{Li_2003}.

\paragraph{Partially Observed Labels}

\paragraph{Heuristics}
Heuristics can be used to reduce the required
annotation effort [34, 18], but this runs the risk of increasing
error in the labels \cite{mlsp}.
% ======================== Theoretical Background ========================

\section{Background}
Contrary to binary or multi-class classification where each instance is associated with only one label, multi-label classification allows assigning multiple labels to a single input \cite{mlsp}. Given $K$ categories, an input image $x\in \mathcal{X}$ is associated with a binary vector of labels $y=[y_1,...,y_K]$ from the label space $\mathcal{Y}=\{0,1\}^K$, where $y_k=1$ represents that $k$ is present in $x$ and $y_k=0$ otherwise. The goal is to create a model that outputs the probability of the presence of a category $p=[p_1,...,p_K]$ \cite{mlsp,Query2Label}.

\subsection{Challenges in Multi-Label Learning}
\paragraph{Label Correlation}
\paragraph{Class Imbalance}
\paragraph{Sparse Supervision}



\subsection{Convolutional Neural Networks (CNNs)}
In the field of computer vision there exists different deep learning methods that extract features from input images, and the two main neural networks are convolutional neural networks and vision transformers. 



\subsection{Transformer Architectures}
Transformers, introduced by Vaswani et al. \cite{vaswani2023attentionneed}, are a type of neural network architecture initially developed to model long-range dependencies in sequence data. They achieve this using attention and self-attention mechanisms, which enables the model to have a long-term memory of inputs, and dynamically weigh the importance of each element in the input sequence. Originally, transformer-based models was mainly delevoped for natural language processing tasks, but the introduction of the Vision Transformer (ViT) by Dosovitskiy et al. \cite{dosovitskiy2021imageworth16x16words}, transformers have been widely used for computer vision tasks. 

\paragraph{Architecture}
The original transformer consists of an encoder-decoder structure. Both encoder and decoder are made of a stack of identical layers, where encoder layers contain a multi-head self-attention mechanism followed by a position-wise feed-forward network. In addition, decoder layers consists of an encoder-decoder attention layer. 

The transfomer encoder is of interest, as the authors of \cite{Query2Label} use them to extract features and automatically learn label embedding, as described in section \ref{sec:q2l_method}.

\paragraph{Attention}
The core principle of the transformer architecture is the attention mechanism, which allows the model to attend to all posistions in an input sequence when processing each element. Thus alloowing the model to weigh the relevance of different posistions in a sequence. Given a query and a set of key-value pairs, the attention function computes a weighted sum of the values, where the weights are determined by similarities between the query and the keys.

\paragraph{Self-Attention}
The transformer model uses self-attention by relating every element in the input sequence to every other element. The attention function is a function that maps a query and a set of key-value pairs to an output. The scaled dot-product self-attention is defined as:

\begin{equation}
    \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

\noindent where $Q$, $K$, $V$, are the query, key, and value vectors, and $d_k$ is the dimension of the key vectors and serves as a scaling factor \cite{vaswani2023attentionneed}. 

\paragraph{Multi-Head Attention}
The transformer applies multiple attention functions in parallel, allowing the model to attend to information from different parts of the sequence. The embedding is split into multiple heads, perform attention for each, and then concatenate them back together, defined as:

\begin{equation}
    \text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O
\end{equation}
    
\noindent where each head is computed as:

\begin{equation}
    \text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)
\end{equation}

\noindent with projection matrices $W_i^Q$, $W_i^K$, $W_i^V$, and $W_i^O$.

\paragraph{Cross-Attention}
Contrary to self-attention, where queries, keys, and values are generated from the same input, cross-attention is a mechanisn where queries come from one source, and the keys and values come fron another. This mechanism allows the model to relate elements from one input to relevant parts of another input. The authours of Query2Label \cite{Query2Label} make use of this mechanism, where label embeddings (queries) attend to spatial image features (key-value pairs).

\paragraph{Feed-Forward Networks and Positional Encoding}
Each layer in the Transformer also includes a fully connected feed-forward network applied to each position. To compensate for the lack of order in the input sequence, sinusoidal positional encodings are added to the input embeddings. These encodings allow the model to distinguish the order of elements in the sequence using functions of varying frequencies.

\paragraph{Vision Transformers (ViTs)}
The Vision Transformer (ViT), introduced by Dosovitskiy et al. \cite{dosovitskiy2021imageworth16x16words}, is a model for image classification that leverages the transformer architecture: the self-attention mechanism of transformers that allows the model to selectively weigh the significance of each part of the input, and positional embeddings that represent the order of tokens in a sequence. In ViTs, images are represented as sequences, where the label function as a learnable token for classification. The input image is divided into a sequence of patches that are flattened and linearly embedded into a vector. The spatial information is preserved by adding positional encodings to the embeddings. The resulting sequence is fed into a transformer decoder identical to that introduced in \cite{vaswani2023attentionneed} to model global relations for classification.

\subsection{Loss Functions}
Loss functions are a fundamental component in deep learning, as they serve as a measure of how much the model predictions deviate from the ground truth \cite{zhang2023dive}.

\paragraph{Binary Cross-Entropy}
\paragraph{Focal Loss}
\paragraph{Assymetric Loss}

\paragraph{Online Estimation of Unobserved Labels (ROLE)}

\begin{equation}
    \mathcal{L}_{ROLE}(\bm{F}_B, \bm{\tilde{Y}}_B) = \frac{\mathcal{L}'(\bm{F}_B)|\bm{\tilde{Y}}+\mathcal{L}'(\bm{\tilde{Y}}|\bm{F}_B)}{2}
\end{equation}

for a batch $B$, where $\bm{\tilde{Y}}\in[0,1]^{N\times L}$ represent the estimated labels, and $\bm{F}\in[0,1]^{N\times L}$ is the matrix of classifier predictions.

The goal is to train the label estimator $g(\cdot;\phi)$ and the image classifier $f(\cdot;\theta)$



% ======================== Methodology ========================

\section{Method}
% Focus Methodology on the rationale for choosing these methods, how the operationalized their training, and any adaptations made for reproduction.

In real-world datasets, obtaining full label annotations is practiaclly impossible.
This project focuses on finding solutions to the multi-label learning problems:
\begin{itemize} 
    \item  The challenge of multi-label learning in scenarios where only a single positive label per image is available durin training (MLSPL).
    \item Label imbalance (Query2Label).
    \item Feature localization (Query2Label).
\end{itemize}

\subsection{Overview of Approach}
A brief description of what we did to solve the multi-label classification problem. 

\subsection{Dataset}
The MS-COCO 2014 \cite{coco14} dataset is used as a benchmark for evaluation both Query2Label and MLSPL. MS-COCO (Microsoft Common Objects in Context) is a large-scale dataset commonly used for object detection, segmentation, and multi-label image classification. COCO consists of 82,081 training images and 80 classes, and a validation set of 40,137 images.

\subsection{Query2Label: A Simple Transformer Way to Multi-Label Classification}
\label{sec:q2l_method}
Query2Label: A Simple Transformer Way to Multi-Label Classification (Query2Label) by Liu et al. \cite{Query2Label} is a two-stage framework for multi-label classification. It uses transformer decoders to extract features with multi-head attentions focusing on different parts of an object category and learn label embeddings from data automatically.

\subsection{Multi-Label Learning from Single Positive Labels}
Multi-Label Learning from Single Positive Labels (MLSPL) by Cole et al. \cite{mlsp}.
How it uses weak supervision and contrastive learning.

% ======================== Experimental Setup  ========================

\subsection{Experiments}
A description of our setup versus the author's setups.

\vspace{1em}

All experiments were run on a single NVIDIA GeForce RTX 3060 GPU (12 GB VRAM), with Python 3.11.8, NVIDIA driver 550.90.07 and CUDA 12.4. 


\paragraph{Query2Label}
All Query2Label experiments were conducted on environment versions cuda==12.4, torch==2.1.0+cu121, torchvision==0.16.0+cu121, python==3.11.8.
We evaluated the Query2Label model using the best performing backbone model, the CvT-w24 backbone, with a $384\time 384$ input resolution, pretrained on the ImageNet-22k dataset, as described by Liu et al. \cite{Query2Label}. For our experiments, we used a pretrained model checkpoint released by the authors, which was trained on the MS-COCO 2024 dataset. The model was tested on the MS-COCO 2024 validation set with a batchsize of 16 and otherwise no fine-tuning or modification.

% \paragraph{Data preparation}
% \paragraph{Hyperparameters}

\paragraph{Multi-Label Learning from Single Positive Labels}
All MLSPL experiments were conducted on environment versions cuda==12.4, torch==2.2.1+cu121, torchvision==0.17.1+cu121, python==3.11.8.
\paragraph{Data preparation}
The dataset preparation for the MS-COCO dataset in MLSPL relies on converting the standard multi-label MS-COCO dataset to a single positive label format, simulating a weakly supervised setting. Cole et al. does this beginning with the fully annotated multi-label image dataset and corrupt it by discarding annotations, simulating single positive training data by randomly selecting one positive label for each training example \cite{mlsp}.

\vspace{1em}

To convert into a single positive label format, the instructions provided by Cole et al. are followed: firsly, both images and annotations for training and validation are downloaded. Secondly, pre-extracted features for COCO, provided by the authors are downloaded. Lastly, the script \texttt{format\_coco.py} is used to produce uniformly formatted image lists and labels.

\vspace{1em}

\paragraph{Hyperparameters}

\subsection{Evaluation Metrics}
To asses model performance, we adopt mean Average Precision (mAP), a standard metric widely reported in multi-label classification tasks as it is used to analyze the performance object detection and segmentation. Both Query2Label and MLSPL report results in terms of mAP. 

% ======================== Results and Discussion ========================

\section{Results and Discussion}
\begin{table*}[!t]
    \small
    \caption{Comparison of mAP results retween our experiments and reported mAP results on the MS-COCO 2014 Dataset.}
    \label{tab:q2l_map_comparison}
    \centering
    \begin{tabular}{l l l c c}
    \toprule
    \textbf{Method} & \textbf{Backbone} & \textbf{Resolution} & \textbf{mAP(Ours)} & \textbf{mAP (Paper)} \\
    \midrule
    Q2L-R101-448 & ResNet-101     & $448\times448$ & 84.9 & 84.9 \\
    Q2L-R101-576 & ResNet-101     & $576\times576$ & 86.5 & 86.5 \\
    Q2L-SwinL    & Swin-L(22k)    & $384\times384$ & 90.5 & 90.5 \\
    Q2L-CvT      & CvT-w24(22k)   & $384\times384$ & 91.3 & 91.3 \\
    \bottomrule
    \end{tabular}
\end{table*}

This section compares results from the experiments to the those of the papers. Discuss why they might not be the same, or describe the similarities. Discuss whether the methods are able to solve the problems.


\subsection{Results from Query2Label}
The results from our experiments are compared to those of the paper in table \ref{tab:q2l_map_comparison}. 

\subsection{Results from Multi-Label Learning from Single Positive Labels}

% ======================== Conclusion ========================

\section{Conclusion}
% [] Summarize findings for each method.

% [] Mention if the results reproduced align with the papers.

% [] Suggest future directions or improvements.


\bibliographystyle{IEEEtran}
\bibliography{references}




\end{document}


